{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sc0les/nba_prediction_markets/blob/main/WIP_Neural_Net_Development_%26_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7TAMjuIJcux"
      },
      "outputs": [],
      "source": [
        "!pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmjVHXhV2zdD"
      },
      "outputs": [],
      "source": [
        "!pip install scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e8EjhbR7fHT",
        "outputId": "d4d2e33a-e449-4181-b190-32e31727f3f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from math import floor\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.layers import LeakyReLU\n",
        "LeakyReLU = LeakyReLU(alpha=0.1)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "364b48dd"
      },
      "outputs": [],
      "source": [
        "full_df = pd.read_csv('/content/drive/My Drive/Sam Colelli BSTN Capstone/Datasets/final_nba_df.csv', index_col = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de7e81bc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "WL_df = full_df.drop(columns = ['OU_hit', 'spread_hit'])\n",
        "\n",
        "X = WL_df.iloc[:, :2818].to_numpy()\n",
        "y = WL_df.iloc[:, 2818:].to_numpy().ravel()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk8y4EOUofUC"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmI1UNHXvZH7"
      },
      "outputs": [],
      "source": [
        "score_acc = make_scorer(accuracy_score)\n",
        "\n",
        "def nn_cl_bo(neurons, activation, optimizer, learning_rate,  batch_size, epochs ):\n",
        "    optimizer_list = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
        "    optimizer_dict = {'Adam': Adam(lr = learning_rate),\n",
        "                 'SGD': SGD(lr = learning_rate),\n",
        "                 'RMSprop': RMSprop(lr = learning_rate),\n",
        "                 'Adadelta': Adadelta(lr = learning_rate),\n",
        "                 'Adagrad': Adagrad(lr = learning_rate),\n",
        "                 'Adamax': Adamax(lr = learning_rate),\n",
        "                 'Nadam': Nadam(lr = learning_rate),\n",
        "                 'Ftrl': Ftrl(lr=learning_rate)\n",
        "                 }\n",
        "    activation_list = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu', 'elu', 'exponential', LeakyReLU, 'relu']\n",
        "    neurons = round(neurons)\n",
        "    activation = activation_list[round(activation)]\n",
        "    batch_size = round(batch_size)\n",
        "    epochs = round(epochs)\n",
        "    def nn_cl_fun():\n",
        "        opt = Adam(lr = learning_rate)\n",
        "        nn = Sequential()\n",
        "        nn.add(Dense(neurons, input_dim = 2818, activation = activation))\n",
        "        nn.add(Dense(neurons, activation = activation))\n",
        "        #although it is a binary classification problem, I saw a big performance boost when changing it to sparse categorical with softmax, as opposed to sigmoid\n",
        "        nn.add(Dense(2, activation='softmax'))\n",
        "        nn.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "        return nn\n",
        "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
        "    nn = KerasClassifier(model=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    kfold = StratifiedKFold(n_splits=10, shuffle = True, random_state=1)\n",
        "    score = cross_val_score(nn, X_train, y_train, scoring = score_acc, cv=kfold, fit_params={'callbacks':[es]}, error_score = 'raise').mean()\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CLMKAGs3o7v",
        "outputId": "2842ba91-93c9-4b0c-f749-51bd32fc2512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | activa... | batch_... |  epochs   | learni... |  neurons  | optimizer |\n",
            "-------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe7000af0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe6fffede60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.4999   \u001b[0m | \u001b[0m3.753    \u001b[0m | \u001b[0m729.3    \u001b[0m | \u001b[0m20.01    \u001b[0m | \u001b[0m0.3093   \u001b[0m | \u001b[0m155.3    \u001b[0m | \u001b[0m0.6464   \u001b[0m |\n",
            "| \u001b[95m2        \u001b[0m | \u001b[95m0.568    \u001b[0m | \u001b[95m1.676    \u001b[0m | \u001b[95m366.5    \u001b[0m | \u001b[95m51.74    \u001b[0m | \u001b[95m0.5434   \u001b[0m | \u001b[95m425.0    \u001b[0m | \u001b[95m4.797    \u001b[0m |\n",
            "| \u001b[95m3        \u001b[0m | \u001b[95m0.5849   \u001b[0m | \u001b[95m1.84     \u001b[0m | \u001b[95m882.0    \u001b[0m | \u001b[95m22.19    \u001b[0m | \u001b[95m0.6738   \u001b[0m | \u001b[95m423.1    \u001b[0m | \u001b[95m3.911    \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m0.4662   \u001b[0m | \u001b[0m1.263    \u001b[0m | \u001b[0m223.8    \u001b[0m | \u001b[0m84.06    \u001b[0m | \u001b[0m0.9686   \u001b[0m | \u001b[0m320.3    \u001b[0m | \u001b[0m4.846    \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m0.5509   \u001b[0m | \u001b[0m7.888    \u001b[0m | \u001b[0m898.0    \u001b[0m | \u001b[0m26.8     \u001b[0m | \u001b[0m0.04866  \u001b[0m | \u001b[0m178.1    \u001b[0m | \u001b[0m6.147    \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m0.5001   \u001b[0m | \u001b[0m0.8851   \u001b[0m | \u001b[0m439.6    \u001b[0m | \u001b[0m96.63    \u001b[0m | \u001b[0m0.5378   \u001b[0m | \u001b[0m695.0    \u001b[0m | \u001b[0m2.209    \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m0.5171   \u001b[0m | \u001b[0m6.179    \u001b[0m | \u001b[0m839.9    \u001b[0m | \u001b[0m21.46    \u001b[0m | \u001b[0m0.7526   \u001b[0m | \u001b[0m989.0    \u001b[0m | \u001b[0m5.237    \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m0.534    \u001b[0m | \u001b[0m2.524    \u001b[0m | \u001b[0m796.0    \u001b[0m | \u001b[0m28.26    \u001b[0m | \u001b[0m0.4534   \u001b[0m | \u001b[0m909.5    \u001b[0m | \u001b[0m2.055    \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m0.5511   \u001b[0m | \u001b[0m2.59     \u001b[0m | \u001b[0m157.9    \u001b[0m | \u001b[0m21.55    \u001b[0m | \u001b[0m0.682    \u001b[0m | \u001b[0m219.5    \u001b[0m | \u001b[0m1.859    \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m0.5002   \u001b[0m | \u001b[0m4.424    \u001b[0m | \u001b[0m83.65    \u001b[0m | \u001b[0m65.93    \u001b[0m | \u001b[0m0.1553   \u001b[0m | \u001b[0m593.4    \u001b[0m | \u001b[0m4.898    \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m0.5002   \u001b[0m | \u001b[0m0.921    \u001b[0m | \u001b[0m432.8    \u001b[0m | \u001b[0m75.55    \u001b[0m | \u001b[0m0.42     \u001b[0m | \u001b[0m59.45    \u001b[0m | \u001b[0m3.751    \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m0.5339   \u001b[0m | \u001b[0m5.974    \u001b[0m | \u001b[0m530.4    \u001b[0m | \u001b[0m95.57    \u001b[0m | \u001b[0m0.5907   \u001b[0m | \u001b[0m904.4    \u001b[0m | \u001b[0m0.9623   \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m0.5679   \u001b[0m | \u001b[0m1.253    \u001b[0m | \u001b[0m813.6    \u001b[0m | \u001b[0m51.81    \u001b[0m | \u001b[0m0.1737   \u001b[0m | \u001b[0m928.2    \u001b[0m | \u001b[0m2.434    \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m0.4151   \u001b[0m | \u001b[0m6.757    \u001b[0m | \u001b[0m734.8    \u001b[0m | \u001b[0m90.66    \u001b[0m | \u001b[0m0.6274   \u001b[0m | \u001b[0m753.4    \u001b[0m | \u001b[0m2.442    \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m0.5849   \u001b[0m | \u001b[0m2.429    \u001b[0m | \u001b[0m899.2    \u001b[0m | \u001b[0m54.25    \u001b[0m | \u001b[0m0.9652   \u001b[0m | \u001b[0m666.8    \u001b[0m | \u001b[0m4.352    \u001b[0m |\n",
            "| \u001b[0m16       \u001b[0m | \u001b[0m0.517    \u001b[0m | \u001b[0m1.033    \u001b[0m | \u001b[0m951.1    \u001b[0m | \u001b[0m55.99    \u001b[0m | \u001b[0m0.5826   \u001b[0m | \u001b[0m414.1    \u001b[0m | \u001b[0m1.659    \u001b[0m |\n",
            "| \u001b[0m17       \u001b[0m | \u001b[0m0.534    \u001b[0m | \u001b[0m8.13     \u001b[0m | \u001b[0m587.3    \u001b[0m | \u001b[0m20.23    \u001b[0m | \u001b[0m0.621    \u001b[0m | \u001b[0m333.4    \u001b[0m | \u001b[0m3.689    \u001b[0m |\n",
            "| \u001b[0m18       \u001b[0m | \u001b[0m0.5336   \u001b[0m | \u001b[0m7.973    \u001b[0m | \u001b[0m377.8    \u001b[0m | \u001b[0m92.68    \u001b[0m | \u001b[0m0.6271   \u001b[0m | \u001b[0m25.66    \u001b[0m | \u001b[0m6.506    \u001b[0m |\n",
            "| \u001b[0m19       \u001b[0m | \u001b[0m0.517    \u001b[0m | \u001b[0m6.218    \u001b[0m | \u001b[0m997.4    \u001b[0m | \u001b[0m33.79    \u001b[0m | \u001b[0m0.1458   \u001b[0m | \u001b[0m933.3    \u001b[0m | \u001b[0m4.878    \u001b[0m |\n",
            "| \u001b[0m20       \u001b[0m | \u001b[0m0.5509   \u001b[0m | \u001b[0m0.594    \u001b[0m | \u001b[0m763.3    \u001b[0m | \u001b[0m80.31    \u001b[0m | \u001b[0m0.9238   \u001b[0m | \u001b[0m714.4    \u001b[0m | \u001b[0m0.8699   \u001b[0m |\n",
            "| \u001b[0m21       \u001b[0m | \u001b[0m0.5339   \u001b[0m | \u001b[0m0.1789   \u001b[0m | \u001b[0m57.37    \u001b[0m | \u001b[0m22.26    \u001b[0m | \u001b[0m0.2537   \u001b[0m | \u001b[0m861.4    \u001b[0m | \u001b[0m3.772    \u001b[0m |\n",
            "| \u001b[0m22       \u001b[0m | \u001b[0m0.551    \u001b[0m | \u001b[0m4.975    \u001b[0m | \u001b[0m847.1    \u001b[0m | \u001b[0m29.93    \u001b[0m | \u001b[0m0.2864   \u001b[0m | \u001b[0m589.9    \u001b[0m | \u001b[0m6.787    \u001b[0m |\n",
            "| \u001b[0m23       \u001b[0m | \u001b[0m0.5511   \u001b[0m | \u001b[0m5.049    \u001b[0m | \u001b[0m50.05    \u001b[0m | \u001b[0m84.05    \u001b[0m | \u001b[0m0.2406   \u001b[0m | \u001b[0m809.0    \u001b[0m | \u001b[0m2.715    \u001b[0m |\n",
            "| \u001b[0m24       \u001b[0m | \u001b[0m0.534    \u001b[0m | \u001b[0m7.772    \u001b[0m | \u001b[0m755.2    \u001b[0m | \u001b[0m64.5     \u001b[0m | \u001b[0m0.1451   \u001b[0m | \u001b[0m69.32    \u001b[0m | \u001b[0m0.8494   \u001b[0m |\n",
            "| \u001b[0m25       \u001b[0m | \u001b[0m0.5849   \u001b[0m | \u001b[0m0.401    \u001b[0m | \u001b[0m136.1    \u001b[0m | \u001b[0m38.06    \u001b[0m | \u001b[0m0.7159   \u001b[0m | \u001b[0m564.1    \u001b[0m | \u001b[0m0.08789  \u001b[0m |\n",
            "| \u001b[0m26       \u001b[0m | \u001b[0m0.5679   \u001b[0m | \u001b[0m6.013    \u001b[0m | \u001b[0m848.5    \u001b[0m | \u001b[0m35.08    \u001b[0m | \u001b[0m0.6336   \u001b[0m | \u001b[0m590.1    \u001b[0m | \u001b[0m0.6776   \u001b[0m |\n",
            "| \u001b[0m27       \u001b[0m | \u001b[0m0.5339   \u001b[0m | \u001b[0m7.731    \u001b[0m | \u001b[0m894.4    \u001b[0m | \u001b[0m20.72    \u001b[0m | \u001b[0m0.1295   \u001b[0m | \u001b[0m435.3    \u001b[0m | \u001b[0m6.993    \u001b[0m |\n",
            "| \u001b[0m28       \u001b[0m | \u001b[0m0.483    \u001b[0m | \u001b[0m6.204    \u001b[0m | \u001b[0m899.3    \u001b[0m | \u001b[0m47.53    \u001b[0m | \u001b[0m0.6633   \u001b[0m | \u001b[0m680.3    \u001b[0m | \u001b[0m6.411    \u001b[0m |\n",
            "| \u001b[0m29       \u001b[0m | \u001b[0m0.5339   \u001b[0m | \u001b[0m5.222    \u001b[0m | \u001b[0m873.1    \u001b[0m | \u001b[0m23.2     \u001b[0m | \u001b[0m0.9018   \u001b[0m | \u001b[0m420.5    \u001b[0m | \u001b[0m5.067    \u001b[0m |\n",
            "=================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# setting up parameters for GridSearch\n",
        "params_nn ={\n",
        "    'neurons': (10, 1000),\n",
        "    'activation': (0, 9),\n",
        "    'optimizer': (0,7),\n",
        "    'learning_rate': (0.01, 1),\n",
        "    'batch_size': (32, 1000),\n",
        "    'epochs': (20, 100)\n",
        "}\n",
        "# Attempting to combine GridSearch hyperparameter selection and Bayesian Optimization so that I can save on computational time and cost\n",
        "nn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=1)\n",
        "nn_bo.maximize(init_points=25, n_iter=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF6WKHjFINdG",
        "outputId": "4ea80139-32ea-4210-c70f-aabed019f517"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'activation': 'softplus',\n",
              " 'batch_size': 882.0176784264352,\n",
              " 'epochs': 22.191007455834093,\n",
              " 'learning_rate': 0.6737628350766182,\n",
              " 'neurons': 423.1317543434557,\n",
              " 'optimizer': 3.9108287991202615}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#displaying hyperparameters\n",
        "params_nn_ = nn_bo.max['params']\n",
        "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu', 'elu', 'exponential', LeakyReLU, 'relu']\n",
        "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
        "params_nn_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbdFDErvg2N9"
      },
      "outputs": [],
      "source": [
        "# Creating a second function to take the high-level hyperparameters to determine additional optimal parameters, such as number of hidden layers and dropoutrate\n",
        "def nn_cl_bo2(neurons, activation, optimizer, learning_rate, batch_size, epochs, layers1, layers2, normalization, dropout, dropout_rate):\n",
        "    optimizer_list = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
        "    optimizer_dict = {'Adam': Adam(lr = learning_rate),\n",
        "                 'SGD': SGD(lr = learning_rate),\n",
        "                 'RMSprop': RMSprop(lr = learning_rate),\n",
        "                 'Adadelta': Adadelta(lr = learning_rate),\n",
        "                 'Adagrad': Adagrad(lr = learning_rate),\n",
        "                 'Adamax': Adamax(lr = learning_rate),\n",
        "                 'Nadam': Nadam(lr = learning_rate),\n",
        "                 'Ftrl': Ftrl(lr = learning_rate)}\n",
        "    activation_list = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu', 'elu', 'exponential', LeakyReLU,'relu']\n",
        "    neurons = round(neurons)\n",
        "    activation = activation_list[round(activation)]\n",
        "    optimizer = optimizer_dict[optimizer_list[round(optimizer)]]\n",
        "    batch_size = round(batch_size)\n",
        "    epochs = round(epochs)\n",
        "    layers1 = round(layers1)\n",
        "    layers2 = round(layers2)\n",
        "    def nn_cl_fun():\n",
        "        nn = Sequential()\n",
        "        nn.add(Dense(neurons, input_dim = 2818, activation=activation))\n",
        "        if normalization > 0.5:\n",
        "            nn.add(BatchNormalization())\n",
        "        for i in range(layers1):\n",
        "            nn.add(Dense(neurons, activation=activation))\n",
        "        if dropout > 0.5:\n",
        "            nn.add(Dropout(dropout_rate, seed=123))\n",
        "        for i in range(layers2):\n",
        "            nn.add(Dense(neurons, activation=activation))\n",
        "        nn.add(Dense(2, activation = 'softmax'))\n",
        "        nn.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
        "        return nn\n",
        "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
        "    nn = KerasClassifier(model=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 11)\n",
        "    score = cross_val_score(nn, X_train, y_train, scoring = score_acc, cv = kfold, fit_params = {'callbacks':[es]}, error_score = 'raise').mean()\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlBxI02Dhgv-",
        "outputId": "3ac0e216-ffd7-4de9-801f-ffc312bf065d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | activa... | batch_... |  dropout  | dropou... |  epochs   |  layers1  |  layers2  | learni... |  neurons  | normal... | optimizer |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.517    \u001b[0m | \u001b[0m5.51     \u001b[0m | \u001b[0m195.7    \u001b[0m | \u001b[0m0.4361   \u001b[0m | \u001b[0m0.2308   \u001b[0m | \u001b[0m43.63    \u001b[0m | \u001b[0m1.597    \u001b[0m | \u001b[0m1.09     \u001b[0m | \u001b[0m0.426    \u001b[0m | \u001b[0m473.0    \u001b[0m | \u001b[0m0.3377   \u001b[0m | \u001b[0m6.935    \u001b[0m |\n",
            "| \u001b[95m2        \u001b[0m | \u001b[95m0.5849   \u001b[0m | \u001b[95m2.14     \u001b[0m | \u001b[95m110.6    \u001b[0m | \u001b[95m0.6696   \u001b[0m | \u001b[95m0.1864   \u001b[0m | \u001b[95m41.94    \u001b[0m | \u001b[95m2.865    \u001b[0m | \u001b[95m1.473    \u001b[0m | \u001b[95m0.08322  \u001b[0m | \u001b[95m1.758e+03\u001b[0m | \u001b[95m0.794    \u001b[0m | \u001b[95m5.884    \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m0.4151   \u001b[0m | \u001b[0m7.337    \u001b[0m | \u001b[0m991.2    \u001b[0m | \u001b[0m0.5773   \u001b[0m | \u001b[0m0.2441   \u001b[0m | \u001b[0m53.71    \u001b[0m | \u001b[0m1.11     \u001b[0m | \u001b[0m2.817    \u001b[0m | \u001b[0m0.1143   \u001b[0m | \u001b[0m1.595e+03\u001b[0m | \u001b[0m0.6977   \u001b[0m | \u001b[0m3.957    \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m0.4491   \u001b[0m | \u001b[0m2.468    \u001b[0m | \u001b[0m998.5    \u001b[0m | \u001b[0m0.138    \u001b[0m | \u001b[0m0.1846   \u001b[0m | \u001b[0m58.8     \u001b[0m | \u001b[0m2.62     \u001b[0m | \u001b[0m3.911    \u001b[0m | \u001b[0m0.3296   \u001b[0m | \u001b[0m787.1    \u001b[0m | \u001b[0m0.319    \u001b[0m | \u001b[0m6.631    \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m0.4151   \u001b[0m | \u001b[0m8.268    \u001b[0m | \u001b[0m819.8    \u001b[0m | \u001b[0m0.03408  \u001b[0m | \u001b[0m0.283    \u001b[0m | \u001b[0m96.04    \u001b[0m | \u001b[0m4.226    \u001b[0m | \u001b[0m2.925    \u001b[0m | \u001b[0m0.9671   \u001b[0m | \u001b[0m818.9    \u001b[0m | \u001b[0m0.3188   \u001b[0m | \u001b[0m0.1151   \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m0.483    \u001b[0m | \u001b[0m0.3436   \u001b[0m | \u001b[0m83.39    \u001b[0m | \u001b[0m0.128    \u001b[0m | \u001b[0m0.01001  \u001b[0m | \u001b[0m38.11    \u001b[0m | \u001b[0m3.176    \u001b[0m | \u001b[0m1.715    \u001b[0m | \u001b[0m0.1876   \u001b[0m | \u001b[0m300.2    \u001b[0m | \u001b[0m0.683    \u001b[0m | \u001b[0m3.283    \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m0.4151   \u001b[0m | \u001b[0m6.914    \u001b[0m | \u001b[0m679.4    \u001b[0m | \u001b[0m0.4413   \u001b[0m | \u001b[0m0.1786   \u001b[0m | \u001b[0m56.93    \u001b[0m | \u001b[0m4.853    \u001b[0m | \u001b[0m1.591    \u001b[0m | \u001b[0m0.9077   \u001b[0m | \u001b[0m976.0    \u001b[0m | \u001b[0m0.5925   \u001b[0m | \u001b[0m4.793    \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m0.5283   \u001b[0m | \u001b[0m1.597    \u001b[0m | \u001b[0m869.0    \u001b[0m | \u001b[0m0.4821   \u001b[0m | \u001b[0m0.0208   \u001b[0m | \u001b[0m49.18    \u001b[0m | \u001b[0m2.446    \u001b[0m | \u001b[0m2.888    \u001b[0m | \u001b[0m0.1877   \u001b[0m | \u001b[0m350.0    \u001b[0m | \u001b[0m0.9491   \u001b[0m | \u001b[0m4.59     \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m0.5169   \u001b[0m | \u001b[0m1.215    \u001b[0m | \u001b[0m930.1    \u001b[0m | \u001b[0m0.8418   \u001b[0m | \u001b[0m0.01583  \u001b[0m | \u001b[0m36.29    \u001b[0m | \u001b[0m4.49     \u001b[0m | \u001b[0m3.695    \u001b[0m | \u001b[0m0.3043   \u001b[0m | \u001b[0m1.435e+03\u001b[0m | \u001b[0m0.6183   \u001b[0m | \u001b[0m1.473    \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m0.4151   \u001b[0m | \u001b[0m7.219    \u001b[0m | \u001b[0m89.22    \u001b[0m | \u001b[0m0.3082   \u001b[0m | \u001b[0m0.06221  \u001b[0m | \u001b[0m97.78    \u001b[0m | \u001b[0m4.638    \u001b[0m | \u001b[0m3.706    \u001b[0m | \u001b[0m0.124    \u001b[0m | \u001b[0m1.868e+03\u001b[0m | \u001b[0m0.09171  \u001b[0m | \u001b[0m4.409    \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m0.5509   \u001b[0m | \u001b[0m8.126    \u001b[0m | \u001b[0m360.9    \u001b[0m | \u001b[0m0.6528   \u001b[0m | \u001b[0m0.2775   \u001b[0m | \u001b[0m49.92    \u001b[0m | \u001b[0m4.085    \u001b[0m | \u001b[0m4.585    \u001b[0m | \u001b[0m0.624    \u001b[0m | \u001b[0m303.1    \u001b[0m | \u001b[0m0.3749   \u001b[0m | \u001b[0m4.451    \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m0.5849   \u001b[0m | \u001b[0m4.132    \u001b[0m | \u001b[0m547.2    \u001b[0m | \u001b[0m0.3523   \u001b[0m | \u001b[0m0.198    \u001b[0m | \u001b[0m58.12    \u001b[0m | \u001b[0m2.818    \u001b[0m | \u001b[0m1.499    \u001b[0m | \u001b[0m0.4183   \u001b[0m | \u001b[0m539.8    \u001b[0m | \u001b[0m0.3467   \u001b[0m | \u001b[0m6.821    \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m0.5509   \u001b[0m | \u001b[0m1.94     \u001b[0m | \u001b[0m693.0    \u001b[0m | \u001b[0m0.03181  \u001b[0m | \u001b[0m0.2506   \u001b[0m | \u001b[0m76.13    \u001b[0m | \u001b[0m4.864    \u001b[0m | \u001b[0m3.367    \u001b[0m | \u001b[0m0.2252   \u001b[0m | \u001b[0m1.405e+03\u001b[0m | \u001b[0m0.03087  \u001b[0m | \u001b[0m2.931    \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m0.4491   \u001b[0m | \u001b[0m2.531    \u001b[0m | \u001b[0m134.8    \u001b[0m | \u001b[0m0.4263   \u001b[0m | \u001b[0m0.2522   \u001b[0m | \u001b[0m28.83    \u001b[0m | \u001b[0m4.947    \u001b[0m | \u001b[0m1.933    \u001b[0m | \u001b[0m0.7242   \u001b[0m | \u001b[0m1.292e+03\u001b[0m | \u001b[0m0.07776  \u001b[0m | \u001b[0m4.881    \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m0.5169   \u001b[0m | \u001b[0m2.388    \u001b[0m | \u001b[0m905.0    \u001b[0m | \u001b[0m0.8183   \u001b[0m | \u001b[0m0.1198   \u001b[0m | \u001b[0m85.62    \u001b[0m | \u001b[0m1.791    \u001b[0m | \u001b[0m3.09     \u001b[0m | \u001b[0m0.4184   \u001b[0m | \u001b[0m1.806e+03\u001b[0m | \u001b[0m0.8254   \u001b[0m | \u001b[0m3.507    \u001b[0m |\n",
            "| \u001b[0m16       \u001b[0m | \u001b[0m0.5849   \u001b[0m | \u001b[0m1.051    \u001b[0m | \u001b[0m43.22    \u001b[0m | \u001b[0m0.9132   \u001b[0m | \u001b[0m0.1537   \u001b[0m | \u001b[0m87.45    \u001b[0m | \u001b[0m1.379    \u001b[0m | \u001b[0m4.214    \u001b[0m | \u001b[0m0.07161  \u001b[0m | \u001b[0m1.243e+03\u001b[0m | \u001b[0m0.9688   \u001b[0m | \u001b[0m2.782    \u001b[0m |\n",
            "| \u001b[0m17       \u001b[0m | \u001b[0m0.551    \u001b[0m | \u001b[0m5.936    \u001b[0m | \u001b[0m240.0    \u001b[0m | \u001b[0m0.8899   \u001b[0m | \u001b[0m0.296    \u001b[0m | \u001b[0m79.09    \u001b[0m | \u001b[0m3.566    \u001b[0m | \u001b[0m2.009    \u001b[0m | \u001b[0m0.4811   \u001b[0m | \u001b[0m530.0    \u001b[0m | \u001b[0m0.8683   \u001b[0m | \u001b[0m1.868    \u001b[0m |\n",
            "| \u001b[0m18       \u001b[0m | \u001b[0m0.5169   \u001b[0m | \u001b[0m8.757    \u001b[0m | \u001b[0m238.7    \u001b[0m | \u001b[0m0.2978   \u001b[0m | \u001b[0m0.221    \u001b[0m | \u001b[0m21.03    \u001b[0m | \u001b[0m1.121    \u001b[0m | \u001b[0m3.935    \u001b[0m | \u001b[0m0.5033   \u001b[0m | \u001b[0m433.1    \u001b[0m | \u001b[0m0.00893  \u001b[0m | \u001b[0m5.955    \u001b[0m |\n",
            "| \u001b[0m19       \u001b[0m | \u001b[0m0.517    \u001b[0m | \u001b[0m4.828    \u001b[0m | \u001b[0m732.4    \u001b[0m | \u001b[0m0.6616   \u001b[0m | \u001b[0m0.2516   \u001b[0m | \u001b[0m51.06    \u001b[0m | \u001b[0m2.705    \u001b[0m | \u001b[0m4.313    \u001b[0m | \u001b[0m0.4743   \u001b[0m | \u001b[0m1.601e+03\u001b[0m | \u001b[0m0.01418  \u001b[0m | \u001b[0m2.777    \u001b[0m |\n",
            "| \u001b[0m20       \u001b[0m | \u001b[0m0.517    \u001b[0m | \u001b[0m1.155    \u001b[0m | \u001b[0m146.3    \u001b[0m | \u001b[0m0.206    \u001b[0m | \u001b[0m0.2243   \u001b[0m | \u001b[0m94.41    \u001b[0m | \u001b[0m2.523    \u001b[0m | \u001b[0m2.842    \u001b[0m | \u001b[0m0.8746   \u001b[0m | \u001b[0m1.59e+03 \u001b[0m | \u001b[0m0.02497  \u001b[0m | \u001b[0m6.111    \u001b[0m |\n",
            "| \u001b[0m21       \u001b[0m | \u001b[0m0.483    \u001b[0m | \u001b[0m5.441    \u001b[0m | \u001b[0m531.9    \u001b[0m | \u001b[0m0.5893   \u001b[0m | \u001b[0m0.2399   \u001b[0m | \u001b[0m33.86    \u001b[0m | \u001b[0m1.747    \u001b[0m | \u001b[0m2.033    \u001b[0m | \u001b[0m0.06056  \u001b[0m | \u001b[0m1.082e+03\u001b[0m | \u001b[0m0.3518   \u001b[0m | \u001b[0m6.419    \u001b[0m |\n",
            "| \u001b[0m22       \u001b[0m | \u001b[0m0.5849   \u001b[0m | \u001b[0m4.289    \u001b[0m | \u001b[0m133.2    \u001b[0m | \u001b[0m0.1525   \u001b[0m | \u001b[0m0.08206  \u001b[0m | \u001b[0m82.52    \u001b[0m | \u001b[0m2.572    \u001b[0m | \u001b[0m4.196    \u001b[0m | \u001b[0m0.4387   \u001b[0m | \u001b[0m168.2    \u001b[0m | \u001b[0m0.01064  \u001b[0m | \u001b[0m3.016    \u001b[0m |\n",
            "| \u001b[0m23       \u001b[0m | \u001b[0m0.5849   \u001b[0m | \u001b[0m5.966    \u001b[0m | \u001b[0m530.7    \u001b[0m | \u001b[0m0.5801   \u001b[0m | \u001b[0m0.1479   \u001b[0m | \u001b[0m79.24    \u001b[0m | \u001b[0m4.158    \u001b[0m | \u001b[0m4.124    \u001b[0m | \u001b[0m0.1363   \u001b[0m | \u001b[0m1.834e+03\u001b[0m | \u001b[0m0.8777   \u001b[0m | \u001b[0m4.897    \u001b[0m |\n",
            "| \u001b[0m24       \u001b[0m | \u001b[0m0.485    \u001b[0m | \u001b[0m8.432    \u001b[0m | \u001b[0m684.2    \u001b[0m | \u001b[0m0.5944   \u001b[0m | \u001b[0m0.1035   \u001b[0m | \u001b[0m26.69    \u001b[0m | \u001b[0m3.317    \u001b[0m | \u001b[0m1.07     \u001b[0m | \u001b[0m0.5569   \u001b[0m | \u001b[0m1.237e+03\u001b[0m | \u001b[0m0.6784   \u001b[0m | \u001b[0m1.194    \u001b[0m |\n",
            "| \u001b[0m25       \u001b[0m | \u001b[0m0.517    \u001b[0m | \u001b[0m5.194    \u001b[0m | \u001b[0m231.4    \u001b[0m | \u001b[0m0.2515   \u001b[0m | \u001b[0m0.2908   \u001b[0m | \u001b[0m91.73    \u001b[0m | \u001b[0m1.491    \u001b[0m | \u001b[0m4.525    \u001b[0m | \u001b[0m0.9485   \u001b[0m | \u001b[0m902.2    \u001b[0m | \u001b[0m0.413    \u001b[0m | \u001b[0m4.04     \u001b[0m |\n",
            "| \u001b[0m26       \u001b[0m | \u001b[0m0.5849   \u001b[0m | \u001b[0m1.614    \u001b[0m | \u001b[0m146.1    \u001b[0m | \u001b[0m0.6071   \u001b[0m | \u001b[0m0.2601   \u001b[0m | \u001b[0m61.72    \u001b[0m | \u001b[0m3.761    \u001b[0m | \u001b[0m3.943    \u001b[0m | \u001b[0m0.912    \u001b[0m | \u001b[0m153.0    \u001b[0m | \u001b[0m0.8043   \u001b[0m | \u001b[0m4.947    \u001b[0m |\n",
            "| \u001b[0m27       \u001b[0m | \u001b[0m0.4151   \u001b[0m | \u001b[0m4.939    \u001b[0m | \u001b[0m97.19    \u001b[0m | \u001b[0m0.3655   \u001b[0m | \u001b[0m0.2861   \u001b[0m | \u001b[0m24.8     \u001b[0m | \u001b[0m1.117    \u001b[0m | \u001b[0m4.347    \u001b[0m | \u001b[0m0.8194   \u001b[0m | \u001b[0m1.698e+03\u001b[0m | \u001b[0m0.0947   \u001b[0m | \u001b[0m0.4298   \u001b[0m |\n",
            "| \u001b[0m28       \u001b[0m | \u001b[0m0.483    \u001b[0m | \u001b[0m5.505    \u001b[0m | \u001b[0m243.8    \u001b[0m | \u001b[0m0.6291   \u001b[0m | \u001b[0m0.161    \u001b[0m | \u001b[0m83.41    \u001b[0m | \u001b[0m4.8      \u001b[0m | \u001b[0m1.125    \u001b[0m | \u001b[0m0.5092   \u001b[0m | \u001b[0m527.2    \u001b[0m | \u001b[0m0.8539   \u001b[0m | \u001b[0m2.641    \u001b[0m |\n",
            "| \u001b[0m29       \u001b[0m | \u001b[0m0.4151   \u001b[0m | \u001b[0m7.191    \u001b[0m | \u001b[0m959.1    \u001b[0m | \u001b[0m0.04053  \u001b[0m | \u001b[0m0.2721   \u001b[0m | \u001b[0m78.01    \u001b[0m | \u001b[0m1.814    \u001b[0m | \u001b[0m3.925    \u001b[0m | \u001b[0m0.2568   \u001b[0m | \u001b[0m722.1    \u001b[0m | \u001b[0m0.2389   \u001b[0m | \u001b[0m4.9      \u001b[0m |\n",
            "=============================================================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "params_nn2 ={\n",
        "    'neurons': (10, 1000),\n",
        "    'activation': (0, 9),\n",
        "    'optimizer': (0,7),\n",
        "    'learning_rate': (0.01, 1),\n",
        "    'batch_size': (32, 1000),\n",
        "    'epochs': (20, 100),\n",
        "    'layers1': (1, 4),\n",
        "    'layers2': (1, 4),\n",
        "    'normalization': (0,1),\n",
        "    'dropout': (0,1),\n",
        "    'dropout_rate': (0,0.3)\n",
        "}\n",
        "\n",
        "nn_bo = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=111)\n",
        "nn_bo.maximize(init_points=25, n_iter=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AG-zpaYg3UH",
        "outputId": "1cdcab73-4022-4e05-d367-2d4b6b31fb6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'activation': 'softplus',\n",
              " 'batch_size': 111,\n",
              " 'dropout': 0.6696002382466298,\n",
              " 'dropout_rate': 0.1863728758202091,\n",
              " 'epochs': 42,\n",
              " 'layers1': 2,\n",
              " 'layers2': 1,\n",
              " 'learning_rate': 0.08321798850177216,\n",
              " 'neurons': 902,\n",
              " 'normalization': 0.7939625604796284,\n",
              " 'optimizer': <keras.optimizers.optimizer_v2.nadam.Nadam at 0x7fe700160790>}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#mapping the second optimized GridSearch to the parameters that'll be used for model build and fit\n",
        "params_nn_ = nn_bo.max['params']\n",
        "learning_rate = params_nn_['learning_rate']\n",
        "activation_list = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu', 'elu', 'exponential', LeakyReLU, 'relu']\n",
        "params_nn_['activation'] = activation_list[round(params_nn_['activation'])]\n",
        "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
        "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
        "params_nn_['layers1'] = round(params_nn_['layers1'])\n",
        "params_nn_['layers2'] = round(params_nn_['layers2'])\n",
        "params_nn_['neurons'] = round(params_nn_['neurons'])\n",
        "optimizer_list = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','Adam']\n",
        "optimizer_dict = {'Adam': Adam(lr = learning_rate),\n",
        "                  'SGD': SGD(lr = learning_rate),\n",
        "                  'RMSprop': RMSprop(lr = learning_rate),\n",
        "                  'Adadelta': Adadelta(lr = learning_rate),\n",
        "                  'Adagrad': Adagrad(lr = learning_rate),\n",
        "                  'Adamax': Adamax(lr = learning_rate),\n",
        "                  'Nadam': Nadam(lr = learning_rate),\n",
        "                  'Ftrl': Ftrl(lr = learning_rate)\n",
        "                  }\n",
        "params_nn_['optimizer'] = optimizer_dict[optimizer_list[round(params_nn_['optimizer'])]]\n",
        "params_nn_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ipa8aNAg3gd",
        "outputId": "e922b555-85cd-4bff-e13b-71728cb543c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/42\n",
            "138/138 [==============================] - 3s 10ms/step - loss: 2598938.0000 - accuracy: 0.5720 - val_loss: 0.6806 - val_accuracy: 0.5912\n",
            "Epoch 2/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.7036 - accuracy: 0.5790 - val_loss: 0.9706 - val_accuracy: 0.5912\n",
            "Epoch 3/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 1.5490 - accuracy: 0.5793 - val_loss: 0.6770 - val_accuracy: 0.5912\n",
            "Epoch 4/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6929 - accuracy: 0.5798 - val_loss: 0.6832 - val_accuracy: 0.5836\n",
            "Epoch 5/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 1.4213 - accuracy: 0.5794 - val_loss: 0.6774 - val_accuracy: 0.5912\n",
            "Epoch 6/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.8359 - accuracy: 0.5798 - val_loss: 0.6874 - val_accuracy: 0.5870\n",
            "Epoch 7/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6887 - accuracy: 0.5810 - val_loss: 0.6826 - val_accuracy: 0.5912\n",
            "Epoch 8/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6897 - accuracy: 0.5808 - val_loss: 0.6790 - val_accuracy: 0.5912\n",
            "Epoch 9/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 1.9030 - accuracy: 0.5780 - val_loss: 0.6797 - val_accuracy: 0.5912\n",
            "Epoch 10/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.7998 - accuracy: 0.5767 - val_loss: 0.6763 - val_accuracy: 0.5912\n",
            "Epoch 11/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 1.2895 - accuracy: 0.5783 - val_loss: 0.6765 - val_accuracy: 0.5912\n",
            "Epoch 12/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.7635 - accuracy: 0.5822 - val_loss: 0.6795 - val_accuracy: 0.5912\n",
            "Epoch 13/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6961 - accuracy: 0.5788 - val_loss: 0.6761 - val_accuracy: 0.5912\n",
            "Epoch 14/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 2.6827 - accuracy: 0.5764 - val_loss: 0.6832 - val_accuracy: 0.5912\n",
            "Epoch 15/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 1.4143 - accuracy: 0.5785 - val_loss: 0.6827 - val_accuracy: 0.5912\n",
            "Epoch 16/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6909 - accuracy: 0.5793 - val_loss: 0.6789 - val_accuracy: 0.5922\n",
            "Epoch 17/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 3.5744 - accuracy: 0.5785 - val_loss: 0.8499 - val_accuracy: 0.5912\n",
            "Epoch 18/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.7754 - accuracy: 0.5686 - val_loss: 0.6800 - val_accuracy: 0.5917\n",
            "Epoch 19/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6994 - accuracy: 0.5780 - val_loss: 0.6788 - val_accuracy: 0.5917\n",
            "Epoch 20/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 7.6731 - accuracy: 0.5790 - val_loss: 0.6821 - val_accuracy: 0.5912\n",
            "Epoch 21/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 5.6934 - accuracy: 0.5727 - val_loss: 0.6787 - val_accuracy: 0.5857\n",
            "Epoch 22/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.7163 - accuracy: 0.5748 - val_loss: 0.6851 - val_accuracy: 0.5912\n",
            "Epoch 23/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.7723 - accuracy: 0.5755 - val_loss: 0.6767 - val_accuracy: 0.5912\n",
            "Epoch 24/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6885 - accuracy: 0.5797 - val_loss: 0.6910 - val_accuracy: 0.5912\n",
            "Epoch 25/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.8069 - accuracy: 0.5768 - val_loss: 0.6828 - val_accuracy: 0.5912\n",
            "Epoch 26/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6892 - accuracy: 0.5756 - val_loss: 0.6756 - val_accuracy: 0.5912\n",
            "Epoch 27/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.9785 - accuracy: 0.5721 - val_loss: 0.8158 - val_accuracy: 0.5912\n",
            "Epoch 28/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.7148 - accuracy: 0.5547 - val_loss: 0.6747 - val_accuracy: 0.5912\n",
            "Epoch 29/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 1.1387 - accuracy: 0.5677 - val_loss: 0.6970 - val_accuracy: 0.4807\n",
            "Epoch 30/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6913 - accuracy: 0.5592 - val_loss: 0.6783 - val_accuracy: 0.5912\n",
            "Epoch 31/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6895 - accuracy: 0.5626 - val_loss: 0.6757 - val_accuracy: 0.5912\n",
            "Epoch 32/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 4.3752 - accuracy: 0.5712 - val_loss: 0.6798 - val_accuracy: 0.5912\n",
            "Epoch 33/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 1.1325 - accuracy: 0.5681 - val_loss: 6.9006 - val_accuracy: 0.5912\n",
            "Epoch 34/42\n",
            "138/138 [==============================] - 1s 11ms/step - loss: 1.3203 - accuracy: 0.5728 - val_loss: 0.6786 - val_accuracy: 0.5912\n",
            "Epoch 35/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.7499 - accuracy: 0.5674 - val_loss: 0.6784 - val_accuracy: 0.5912\n",
            "Epoch 36/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6846 - accuracy: 0.5732 - val_loss: 0.6732 - val_accuracy: 0.5912\n",
            "Epoch 37/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6889 - accuracy: 0.5685 - val_loss: 0.6752 - val_accuracy: 0.5912\n",
            "Epoch 38/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6823 - accuracy: 0.5702 - val_loss: 0.6737 - val_accuracy: 0.5912\n",
            "Epoch 39/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6825 - accuracy: 0.5686 - val_loss: 0.6864 - val_accuracy: 0.4979\n",
            "Epoch 40/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 1.0379 - accuracy: 0.5693 - val_loss: 0.6756 - val_accuracy: 0.5912\n",
            "Epoch 41/42\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6822 - accuracy: 0.5704 - val_loss: 0.6997 - val_accuracy: 0.4976\n",
            "Epoch 42/42\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 1.6509 - accuracy: 0.5728 - val_loss: 0.6766 - val_accuracy: 0.5912\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "KerasClassifier(\n",
              "\tmodel=None\n",
              "\tbuild_fn=<function nn_cl_fun at 0x7fe6ff7a4d40>\n",
              "\twarm_start=False\n",
              "\trandom_state=None\n",
              "\toptimizer=rmsprop\n",
              "\tloss=None\n",
              "\tmetrics=None\n",
              "\tbatch_size=111\n",
              "\tvalidation_batch_size=None\n",
              "\tverbose=0\n",
              "\tcallbacks=None\n",
              "\tvalidation_split=0.0\n",
              "\tshuffle=True\n",
              "\trun_eagerly=False\n",
              "\tepochs=42\n",
              "\tclass_weight=None\n",
              ")"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#function for fitting the model to the optimized hyperparameters\n",
        "def nn_cl_fun():\n",
        "  nn = Sequential()\n",
        "  nn.add(Dense(params_nn_['neurons'], input_dim = 2818, activation = params_nn_['activation']))\n",
        "  nn.add(Dense(params_nn_['neurons'], input_dim = 2818, activation = params_nn_['activation']))\n",
        "  if params_nn_['normalization'] > 0.5:\n",
        "    nn.add(BatchNormalization())\n",
        "  for i in range(params_nn_['layers1']):\n",
        "    nn.add(Dense(params_nn_['neurons'], activation = params_nn_['activation']))\n",
        "  if params_nn_['dropout'] > 0.5:\n",
        "    nn.add(Dropout(params_nn_['dropout_rate'], seed=123))\n",
        "  for i in range(params_nn_['layers2']):\n",
        "    nn.add(Dense(params_nn_['neurons'], activation = params_nn_['activation']))\n",
        "  nn.add(Dense(2, activation='softmax'))\n",
        "  nn.compile(loss='sparse_categorical_crossentropy', optimizer = params_nn_['optimizer'], metrics = ['accuracy'])\n",
        "  return nn\n",
        "\n",
        "es = EarlyStopping(monitor = 'accuracy', mode = 'max', verbose = 0, patience = 20)\n",
        "nn = KerasClassifier(build_fn = nn_cl_fun, epochs = params_nn_['epochs'], batch_size = params_nn_['batch_size'], verbose = 0)\n",
        "nn.fit(X_train_scaled, y_train, validation_data = (X_test_scaled, y_test), verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVViSggdg4X-"
      },
      "outputs": [],
      "source": [
        "#the remainder of this notebook is trial and error based on the above hyperparameters, combined with general, out-of-the-box layering and optimization functions\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87ced874"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1950, activation= 'elu'))\n",
        "model.add(Dense(1048, activation= 'elu'))\n",
        "model.add(Dropout(0.13))\n",
        "model.add(Dense(512, activation= 'elu'))\n",
        "model.add(Dense(256, activation= 'elu'))\n",
        "model.add(Dense(128, activation= 'elu'))\n",
        "model.add(Dense(2, activation= 'softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aee0b8e6",
        "outputId": "bb614d45-1b5e-47c0-df9e-05049b2ef1c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/55\n",
            "335/335 [==============================] - 3s 6ms/step - loss: 0.7299 - accuracy: 0.6226 - val_loss: 0.6601 - val_accuracy: 0.6199\n",
            "Epoch 2/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.6176 - accuracy: 0.6605 - val_loss: 0.6469 - val_accuracy: 0.6395\n",
            "Epoch 3/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.5930 - accuracy: 0.6846 - val_loss: 0.6195 - val_accuracy: 0.6674\n",
            "Epoch 4/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.5679 - accuracy: 0.7105 - val_loss: 0.6343 - val_accuracy: 0.6761\n",
            "Epoch 5/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.5479 - accuracy: 0.7208 - val_loss: 0.6488 - val_accuracy: 0.6709\n",
            "Epoch 6/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.4918 - accuracy: 0.7570 - val_loss: 0.6469 - val_accuracy: 0.6953\n",
            "Epoch 7/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.4539 - accuracy: 0.7796 - val_loss: 0.6962 - val_accuracy: 0.6593\n",
            "Epoch 8/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.3943 - accuracy: 0.8168 - val_loss: 0.7345 - val_accuracy: 0.6802\n",
            "Epoch 9/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.3213 - accuracy: 0.8572 - val_loss: 0.8773 - val_accuracy: 0.6802\n",
            "Epoch 10/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.2802 - accuracy: 0.8716 - val_loss: 1.0249 - val_accuracy: 0.6794\n",
            "Epoch 11/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.2219 - accuracy: 0.9071 - val_loss: 1.0707 - val_accuracy: 0.6561\n",
            "Epoch 12/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1974 - accuracy: 0.9173 - val_loss: 1.4232 - val_accuracy: 0.6604\n",
            "Epoch 13/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1645 - accuracy: 0.9303 - val_loss: 1.2742 - val_accuracy: 0.6580\n",
            "Epoch 14/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1329 - accuracy: 0.9439 - val_loss: 1.3628 - val_accuracy: 0.6609\n",
            "Epoch 15/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1314 - accuracy: 0.9449 - val_loss: 1.7152 - val_accuracy: 0.6720\n",
            "Epoch 16/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1129 - accuracy: 0.9525 - val_loss: 2.1443 - val_accuracy: 0.6539\n",
            "Epoch 17/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1129 - accuracy: 0.9527 - val_loss: 2.1435 - val_accuracy: 0.6694\n",
            "Epoch 18/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0882 - accuracy: 0.9639 - val_loss: 1.6154 - val_accuracy: 0.6717\n",
            "Epoch 19/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0917 - accuracy: 0.9628 - val_loss: 2.2169 - val_accuracy: 0.6643\n",
            "Epoch 20/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.2417 - accuracy: 0.9039 - val_loss: 2.0712 - val_accuracy: 0.6535\n",
            "Epoch 21/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1791 - accuracy: 0.9116 - val_loss: 1.7953 - val_accuracy: 0.6502\n",
            "Epoch 22/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1373 - accuracy: 0.9301 - val_loss: 2.1114 - val_accuracy: 0.6415\n",
            "Epoch 23/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1226 - accuracy: 0.9422 - val_loss: 2.1268 - val_accuracy: 0.6408\n",
            "Epoch 24/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1022 - accuracy: 0.9480 - val_loss: 2.7303 - val_accuracy: 0.6337\n",
            "Epoch 25/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1029 - accuracy: 0.9460 - val_loss: 2.5908 - val_accuracy: 0.6267\n",
            "Epoch 26/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1049 - accuracy: 0.9476 - val_loss: 1.3129 - val_accuracy: 0.6493\n",
            "Epoch 27/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1104 - accuracy: 0.9402 - val_loss: 2.1016 - val_accuracy: 0.6572\n",
            "Epoch 28/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1051 - accuracy: 0.9445 - val_loss: 2.5903 - val_accuracy: 0.6567\n",
            "Epoch 29/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1556 - accuracy: 0.9302 - val_loss: 2.0452 - val_accuracy: 0.6056\n",
            "Epoch 30/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.2122 - accuracy: 0.9055 - val_loss: 1.7489 - val_accuracy: 0.6452\n",
            "Epoch 31/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1244 - accuracy: 0.9376 - val_loss: 1.8893 - val_accuracy: 0.6175\n",
            "Epoch 32/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1027 - accuracy: 0.9449 - val_loss: 2.3632 - val_accuracy: 0.6478\n",
            "Epoch 33/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0872 - accuracy: 0.9516 - val_loss: 2.9886 - val_accuracy: 0.6397\n",
            "Epoch 34/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0934 - accuracy: 0.9487 - val_loss: 2.6567 - val_accuracy: 0.6263\n",
            "Epoch 35/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0989 - accuracy: 0.9490 - val_loss: 2.3240 - val_accuracy: 0.6482\n",
            "Epoch 36/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0893 - accuracy: 0.9507 - val_loss: 2.6171 - val_accuracy: 0.6245\n",
            "Epoch 37/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1101 - accuracy: 0.9481 - val_loss: 3.0265 - val_accuracy: 0.6467\n",
            "Epoch 38/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1254 - accuracy: 0.9398 - val_loss: 3.1081 - val_accuracy: 0.6491\n",
            "Epoch 39/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1448 - accuracy: 0.9338 - val_loss: 2.0440 - val_accuracy: 0.6513\n",
            "Epoch 40/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0954 - accuracy: 0.9493 - val_loss: 3.4225 - val_accuracy: 0.6450\n",
            "Epoch 41/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0878 - accuracy: 0.9530 - val_loss: 3.3246 - val_accuracy: 0.6578\n",
            "Epoch 42/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1051 - accuracy: 0.9477 - val_loss: 2.6524 - val_accuracy: 0.6430\n",
            "Epoch 43/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0867 - accuracy: 0.9507 - val_loss: 2.5191 - val_accuracy: 0.6526\n",
            "Epoch 44/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1947 - accuracy: 0.9405 - val_loss: 17.3060 - val_accuracy: 0.6387\n",
            "Epoch 45/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.4711 - accuracy: 0.8475 - val_loss: 1.6034 - val_accuracy: 0.6448\n",
            "Epoch 46/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.2621 - accuracy: 0.9127 - val_loss: 1.1107 - val_accuracy: 0.6487\n",
            "Epoch 47/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.2266 - accuracy: 0.9292 - val_loss: 1.6508 - val_accuracy: 0.6306\n",
            "Epoch 48/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.2156 - accuracy: 0.9338 - val_loss: 1.9186 - val_accuracy: 0.6363\n",
            "Epoch 49/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.2172 - accuracy: 0.9341 - val_loss: 1.6279 - val_accuracy: 0.6387\n",
            "Epoch 50/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.2119 - accuracy: 0.9359 - val_loss: 1.9722 - val_accuracy: 0.6323\n",
            "Epoch 51/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.2232 - accuracy: 0.9308 - val_loss: 2.1133 - val_accuracy: 0.6397\n",
            "Epoch 52/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.3138 - accuracy: 0.9160 - val_loss: 1.4296 - val_accuracy: 0.6317\n",
            "Epoch 53/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.2149 - accuracy: 0.9355 - val_loss: 1.7273 - val_accuracy: 0.6465\n",
            "Epoch 54/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1991 - accuracy: 0.9405 - val_loss: 1.5492 - val_accuracy: 0.6535\n",
            "Epoch 55/55\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1967 - accuracy: 0.9422 - val_loss: 1.3079 - val_accuracy: 0.6556\n",
            "120/120 [==============================] - 0s 3ms/step - loss: 1.2535 - accuracy: 0.6583\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs = 55, validation_split = 0.3)\n",
        "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsKxhh2419Iq"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(1048, activation= 'relu'))\n",
        "model.add(Dense(512, activation= 'relu'))\n",
        "model.add(Dense(256, activation= 'relu'))\n",
        "model.add(Dense(128, activation= 'relu'))\n",
        "model.add(Dense(2, activation= 'sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3gIZ1ybGO3q",
        "outputId": "753dcbaf-ecc1-4753-9b87-0b4acdaa1ecd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/59\n",
            "335/335 [==============================] - 3s 6ms/step - loss: 0.6760 - accuracy: 0.6303 - val_loss: 0.6164 - val_accuracy: 0.6589\n",
            "Epoch 2/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.6149 - accuracy: 0.6677 - val_loss: 0.6274 - val_accuracy: 0.6380\n",
            "Epoch 3/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.5845 - accuracy: 0.6888 - val_loss: 0.6264 - val_accuracy: 0.6724\n",
            "Epoch 4/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.5438 - accuracy: 0.7263 - val_loss: 0.6483 - val_accuracy: 0.6646\n",
            "Epoch 5/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.4949 - accuracy: 0.7552 - val_loss: 0.6772 - val_accuracy: 0.6717\n",
            "Epoch 6/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.4291 - accuracy: 0.8010 - val_loss: 0.6855 - val_accuracy: 0.6604\n",
            "Epoch 7/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.3552 - accuracy: 0.8383 - val_loss: 0.7556 - val_accuracy: 0.6746\n",
            "Epoch 8/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.2859 - accuracy: 0.8738 - val_loss: 0.8216 - val_accuracy: 0.6648\n",
            "Epoch 9/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.2332 - accuracy: 0.9016 - val_loss: 0.9990 - val_accuracy: 0.6469\n",
            "Epoch 10/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1870 - accuracy: 0.9192 - val_loss: 1.2984 - val_accuracy: 0.6535\n",
            "Epoch 11/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1636 - accuracy: 0.9307 - val_loss: 1.3319 - val_accuracy: 0.6565\n",
            "Epoch 12/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1200 - accuracy: 0.9505 - val_loss: 1.6075 - val_accuracy: 0.6615\n",
            "Epoch 13/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1314 - accuracy: 0.9479 - val_loss: 1.5623 - val_accuracy: 0.6685\n",
            "Epoch 14/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1141 - accuracy: 0.9517 - val_loss: 1.6470 - val_accuracy: 0.6615\n",
            "Epoch 15/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1052 - accuracy: 0.9604 - val_loss: 1.5177 - val_accuracy: 0.6704\n",
            "Epoch 16/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1086 - accuracy: 0.9547 - val_loss: 1.6989 - val_accuracy: 0.6561\n",
            "Epoch 17/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0899 - accuracy: 0.9621 - val_loss: 2.1695 - val_accuracy: 0.6589\n",
            "Epoch 18/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0918 - accuracy: 0.9639 - val_loss: 1.7277 - val_accuracy: 0.6768\n",
            "Epoch 19/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0817 - accuracy: 0.9690 - val_loss: 1.5011 - val_accuracy: 0.6744\n",
            "Epoch 20/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0963 - accuracy: 0.9612 - val_loss: 2.2583 - val_accuracy: 0.6587\n",
            "Epoch 21/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1041 - accuracy: 0.9595 - val_loss: 1.4968 - val_accuracy: 0.6746\n",
            "Epoch 22/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0969 - accuracy: 0.9569 - val_loss: 1.9442 - val_accuracy: 0.6650\n",
            "Epoch 23/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0805 - accuracy: 0.9624 - val_loss: 2.1827 - val_accuracy: 0.6661\n",
            "Epoch 24/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0842 - accuracy: 0.9640 - val_loss: 1.7728 - val_accuracy: 0.6731\n",
            "Epoch 25/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0757 - accuracy: 0.9682 - val_loss: 1.9863 - val_accuracy: 0.6824\n",
            "Epoch 26/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0806 - accuracy: 0.9677 - val_loss: 2.6672 - val_accuracy: 0.6757\n",
            "Epoch 27/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0729 - accuracy: 0.9656 - val_loss: 2.4550 - val_accuracy: 0.6709\n",
            "Epoch 28/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0728 - accuracy: 0.9687 - val_loss: 1.9377 - val_accuracy: 0.6596\n",
            "Epoch 29/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0760 - accuracy: 0.9696 - val_loss: 2.0389 - val_accuracy: 0.6772\n",
            "Epoch 30/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0753 - accuracy: 0.9667 - val_loss: 2.0369 - val_accuracy: 0.6572\n",
            "Epoch 31/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0717 - accuracy: 0.9649 - val_loss: 2.8122 - val_accuracy: 0.6633\n",
            "Epoch 32/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.1010 - accuracy: 0.9523 - val_loss: 2.0504 - val_accuracy: 0.6352\n",
            "Epoch 33/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0760 - accuracy: 0.9616 - val_loss: 2.7400 - val_accuracy: 0.6609\n",
            "Epoch 34/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0787 - accuracy: 0.9709 - val_loss: 2.3300 - val_accuracy: 0.6672\n",
            "Epoch 35/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0802 - accuracy: 0.9688 - val_loss: 3.5050 - val_accuracy: 0.6741\n",
            "Epoch 36/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0714 - accuracy: 0.9713 - val_loss: 2.7175 - val_accuracy: 0.6774\n",
            "Epoch 37/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0593 - accuracy: 0.9764 - val_loss: 1.7699 - val_accuracy: 0.6815\n",
            "Epoch 38/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0604 - accuracy: 0.9770 - val_loss: 2.3924 - val_accuracy: 0.6759\n",
            "Epoch 39/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0591 - accuracy: 0.9747 - val_loss: 2.1877 - val_accuracy: 0.6809\n",
            "Epoch 40/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0576 - accuracy: 0.9761 - val_loss: 2.6432 - val_accuracy: 0.6852\n",
            "Epoch 41/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0723 - accuracy: 0.9733 - val_loss: 1.9195 - val_accuracy: 0.6593\n",
            "Epoch 42/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0558 - accuracy: 0.9774 - val_loss: 2.2415 - val_accuracy: 0.6726\n",
            "Epoch 43/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0631 - accuracy: 0.9700 - val_loss: 1.9239 - val_accuracy: 0.6593\n",
            "Epoch 44/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0565 - accuracy: 0.9771 - val_loss: 2.4710 - val_accuracy: 0.6711\n",
            "Epoch 45/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0469 - accuracy: 0.9807 - val_loss: 2.9226 - val_accuracy: 0.6741\n",
            "Epoch 46/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0730 - accuracy: 0.9683 - val_loss: 2.3624 - val_accuracy: 0.6646\n",
            "Epoch 47/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0520 - accuracy: 0.9792 - val_loss: 2.9515 - val_accuracy: 0.6578\n",
            "Epoch 48/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0475 - accuracy: 0.9798 - val_loss: 2.1215 - val_accuracy: 0.6585\n",
            "Epoch 49/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0479 - accuracy: 0.9800 - val_loss: 2.8160 - val_accuracy: 0.6717\n",
            "Epoch 50/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0630 - accuracy: 0.9743 - val_loss: 2.1658 - val_accuracy: 0.6733\n",
            "Epoch 51/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0566 - accuracy: 0.9799 - val_loss: 2.3832 - val_accuracy: 0.6657\n",
            "Epoch 52/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0676 - accuracy: 0.9718 - val_loss: 2.7367 - val_accuracy: 0.6517\n",
            "Epoch 53/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0681 - accuracy: 0.9709 - val_loss: 3.3963 - val_accuracy: 0.6722\n",
            "Epoch 54/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0441 - accuracy: 0.9800 - val_loss: 2.8585 - val_accuracy: 0.6543\n",
            "Epoch 55/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0490 - accuracy: 0.9821 - val_loss: 3.0151 - val_accuracy: 0.6776\n",
            "Epoch 56/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0755 - accuracy: 0.9666 - val_loss: 2.4388 - val_accuracy: 0.6702\n",
            "Epoch 57/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0612 - accuracy: 0.9703 - val_loss: 2.3633 - val_accuracy: 0.6722\n",
            "Epoch 58/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0542 - accuracy: 0.9787 - val_loss: 2.6825 - val_accuracy: 0.6761\n",
            "Epoch 59/59\n",
            "335/335 [==============================] - 2s 5ms/step - loss: 0.0393 - accuracy: 0.9827 - val_loss: 3.4165 - val_accuracy: 0.6757\n",
            "120/120 [==============================] - 0s 3ms/step - loss: 3.3099 - accuracy: 0.6923\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs = 59, validation_split = 0.3)\n",
        "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CbvpXjO2Zip",
        "outputId": "759130be-5cca-4ee4-d703-dd1f5a007be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/59\n",
            "138/138 [==============================] - 3s 10ms/step - loss: 2456178.5000 - accuracy: 0.5453 - val_loss: 0.7722 - val_accuracy: 0.5802\n",
            "Epoch 2/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.7613 - accuracy: 0.5527 - val_loss: 1.2731 - val_accuracy: 0.5912\n",
            "Epoch 3/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.7244 - accuracy: 0.5536 - val_loss: 4.8944 - val_accuracy: 0.5912\n",
            "Epoch 4/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.7142 - accuracy: 0.5587 - val_loss: 60.1787 - val_accuracy: 0.5110\n",
            "Epoch 5/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 1.0798 - accuracy: 0.5591 - val_loss: 11.2603 - val_accuracy: 0.5073\n",
            "Epoch 6/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.7012 - accuracy: 0.5613 - val_loss: 52.3655 - val_accuracy: 0.5154\n",
            "Epoch 7/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.7048 - accuracy: 0.5656 - val_loss: 1216.6718 - val_accuracy: 0.5912\n",
            "Epoch 8/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 4.7465 - accuracy: 0.5620 - val_loss: 0.7041 - val_accuracy: 0.5912\n",
            "Epoch 9/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.7020 - accuracy: 0.5616 - val_loss: 0.6701 - val_accuracy: 0.5912\n",
            "Epoch 10/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 1.9146 - accuracy: 0.5690 - val_loss: 0.6804 - val_accuracy: 0.5253\n",
            "Epoch 11/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6914 - accuracy: 0.5682 - val_loss: 0.6770 - val_accuracy: 0.5912\n",
            "Epoch 12/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6994 - accuracy: 0.5645 - val_loss: 0.7609 - val_accuracy: 0.5280\n",
            "Epoch 13/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6875 - accuracy: 0.5735 - val_loss: 0.6854 - val_accuracy: 0.5904\n",
            "Epoch 14/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 2.2620 - accuracy: 0.5676 - val_loss: 0.6703 - val_accuracy: 0.5912\n",
            "Epoch 15/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6906 - accuracy: 0.5696 - val_loss: 0.6998 - val_accuracy: 0.5912\n",
            "Epoch 16/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 1.3410 - accuracy: 0.5568 - val_loss: 0.6750 - val_accuracy: 0.5912\n",
            "Epoch 17/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.7070 - accuracy: 0.5505 - val_loss: 0.6836 - val_accuracy: 0.4762\n",
            "Epoch 18/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6985 - accuracy: 0.5535 - val_loss: 0.6843 - val_accuracy: 0.5912\n",
            "Epoch 19/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6922 - accuracy: 0.5583 - val_loss: 0.7053 - val_accuracy: 0.5912\n",
            "Epoch 20/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6926 - accuracy: 0.5573 - val_loss: 0.7105 - val_accuracy: 0.5912\n",
            "Epoch 21/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6896 - accuracy: 0.5629 - val_loss: 0.6756 - val_accuracy: 0.5912\n",
            "Epoch 22/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.9416 - accuracy: 0.5627 - val_loss: 0.6833 - val_accuracy: 0.5912\n",
            "Epoch 23/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 14.1610 - accuracy: 0.5641 - val_loss: 0.6803 - val_accuracy: 0.5912\n",
            "Epoch 24/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6878 - accuracy: 0.5638 - val_loss: 0.8734 - val_accuracy: 0.4846\n",
            "Epoch 25/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6916 - accuracy: 0.5601 - val_loss: 0.6957 - val_accuracy: 0.4848\n",
            "Epoch 26/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6892 - accuracy: 0.5568 - val_loss: 0.6748 - val_accuracy: 0.5912\n",
            "Epoch 27/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6852 - accuracy: 0.5602 - val_loss: 0.6734 - val_accuracy: 0.5912\n",
            "Epoch 28/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6869 - accuracy: 0.5565 - val_loss: 0.6746 - val_accuracy: 0.5912\n",
            "Epoch 29/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6835 - accuracy: 0.5664 - val_loss: 0.6771 - val_accuracy: 0.5912\n",
            "Epoch 30/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6854 - accuracy: 0.5677 - val_loss: 0.6775 - val_accuracy: 0.5912\n",
            "Epoch 31/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6852 - accuracy: 0.5647 - val_loss: 0.6924 - val_accuracy: 0.5912\n",
            "Epoch 32/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6821 - accuracy: 0.5666 - val_loss: 0.6813 - val_accuracy: 0.4830\n",
            "Epoch 33/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6869 - accuracy: 0.5531 - val_loss: 0.6756 - val_accuracy: 0.5912\n",
            "Epoch 34/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6831 - accuracy: 0.5697 - val_loss: 0.6747 - val_accuracy: 0.5912\n",
            "Epoch 35/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6837 - accuracy: 0.5630 - val_loss: 0.6737 - val_accuracy: 0.5912\n",
            "Epoch 36/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6809 - accuracy: 0.5693 - val_loss: 0.6732 - val_accuracy: 0.5912\n",
            "Epoch 37/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 2.1138 - accuracy: 0.5640 - val_loss: 0.6829 - val_accuracy: 0.4854\n",
            "Epoch 38/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6807 - accuracy: 0.5655 - val_loss: 0.6751 - val_accuracy: 0.5912\n",
            "Epoch 39/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6822 - accuracy: 0.5609 - val_loss: 0.7072 - val_accuracy: 0.4841\n",
            "Epoch 40/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6819 - accuracy: 0.5652 - val_loss: 0.6751 - val_accuracy: 0.5909\n",
            "Epoch 41/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6795 - accuracy: 0.5694 - val_loss: 0.6872 - val_accuracy: 0.4843\n",
            "Epoch 42/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6791 - accuracy: 0.5687 - val_loss: 0.6779 - val_accuracy: 0.5912\n",
            "Epoch 43/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.7735 - accuracy: 0.5692 - val_loss: 0.6788 - val_accuracy: 0.5912\n",
            "Epoch 44/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6784 - accuracy: 0.5731 - val_loss: 0.6743 - val_accuracy: 0.5912\n",
            "Epoch 45/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6787 - accuracy: 0.5671 - val_loss: 0.6729 - val_accuracy: 0.5912\n",
            "Epoch 46/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.9997 - accuracy: 0.5724 - val_loss: 0.6710 - val_accuracy: 0.5912\n",
            "Epoch 47/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.7258 - accuracy: 0.5679 - val_loss: 0.6723 - val_accuracy: 0.5912\n",
            "Epoch 48/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 4.6139 - accuracy: 0.5711 - val_loss: 0.6779 - val_accuracy: 0.5912\n",
            "Epoch 49/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 12.8183 - accuracy: 0.5797 - val_loss: 0.6787 - val_accuracy: 0.5912\n",
            "Epoch 50/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 43.0219 - accuracy: 0.5803 - val_loss: 0.6766 - val_accuracy: 0.5912\n",
            "Epoch 51/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 1.3977 - accuracy: 0.5818 - val_loss: 0.6752 - val_accuracy: 0.5912\n",
            "Epoch 52/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6720 - accuracy: 0.5705 - val_loss: 0.6733 - val_accuracy: 0.4927\n",
            "Epoch 53/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6708 - accuracy: 0.5693 - val_loss: 0.6674 - val_accuracy: 0.5912\n",
            "Epoch 54/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6713 - accuracy: 0.5680 - val_loss: 0.6725 - val_accuracy: 0.5912\n",
            "Epoch 55/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6703 - accuracy: 0.5750 - val_loss: 0.6773 - val_accuracy: 0.4877\n",
            "Epoch 56/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 1.0271 - accuracy: 0.5714 - val_loss: 0.6681 - val_accuracy: 0.5912\n",
            "Epoch 57/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6901 - accuracy: 0.5767 - val_loss: 0.6686 - val_accuracy: 0.5912\n",
            "Epoch 58/59\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 3.8370 - accuracy: 0.5704 - val_loss: 0.6735 - val_accuracy: 0.5912\n",
            "Epoch 59/59\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.6786 - accuracy: 0.5805 - val_loss: 0.6974 - val_accuracy: 0.4368\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "KerasClassifier(\n",
              "\tmodel=None\n",
              "\tbuild_fn=<function nn_cl_fun at 0x7fe6ff7a4d40>\n",
              "\twarm_start=False\n",
              "\trandom_state=None\n",
              "\toptimizer=rmsprop\n",
              "\tloss=None\n",
              "\tmetrics=None\n",
              "\tbatch_size=111\n",
              "\tvalidation_batch_size=None\n",
              "\tverbose=0\n",
              "\tcallbacks=None\n",
              "\tvalidation_split=0.0\n",
              "\tshuffle=True\n",
              "\trun_eagerly=False\n",
              "\tepochs=42\n",
              "\tclass_weight=None\n",
              ")"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#relu, softplus\n",
        "#ftrl, adam\n",
        "\n",
        "combined_params = {\n",
        "    'activation': 'relu',\n",
        "    'batch_size': 32,\n",
        "    'dropout': 0.6696002382466298,\n",
        "    'dropout_rate': 0.1863728758202091,\n",
        "    'epochs': 59,\n",
        "    'layers1': 3,\n",
        "    'layers2': 4,\n",
        "    'learning_rate': 0.32958578622954593,\n",
        "    'neurons': 787,\n",
        "    'normalization': 0.3189664036607044,\n",
        "    'optimizer': 'adam'\n",
        "    }\n",
        "\n",
        "def nn_cl_fun():\n",
        "  nn = Sequential()\n",
        "  nn.add(Dense(1950, input_dim=2915, activation=combined_params['activation']))\n",
        "  nn.add(Dense(combined_params['neurons'], activation=combined_params['activation']))\n",
        "  if combined_params['normalization'] > 0.5:\n",
        "    nn.add(BatchNormalization())\n",
        "  for i in range(combined_params['layers1']):\n",
        "    nn.add(Dense(combined_params['neurons'], activation=combined_params['activation']))\n",
        "    nn.add(Dropout(combined_params['dropout_rate']))\n",
        "  for i in range(combined_params['layers2']):\n",
        "    nn.add(Dense(combined_params['neurons'], activation=combined_params['activation']))\n",
        "  nn.add(Dense(2, activation='softmax'))\n",
        "  nn.compile(loss='sparse_categorical_crossentropy', optimizer = combined_params['optimizer'], metrics=['accuracy'])\n",
        "  return nn\n",
        "\n",
        "es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
        "#nn = KerasClassifier(model=nn_cl_fun, epochs=combined_params['epochs'], batch_size=combined_params['batch_size'],verbose=0)\n",
        "nn.fit(X_train_scaled, y_train, epochs=combined_params['epochs'],  validation_data = (X_test_scaled, y_test), verbose=1)\n",
        "#test_loss, test_acc = nn.evaluate(X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oskCLTbe261I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPSqQBWOdadjPpFKIRD7pN7",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
